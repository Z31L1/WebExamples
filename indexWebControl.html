<!DOCTYPE html>
<head>
<meta charset="UTF-8">
</head>
<body>

<canvas id="output" style="padding: 0px; position: fixed; top: 3em; left:50%; transform: translate(-50%);max-height: 25vh;"></canvas>
<h1 id="status" style="padding: 10px; background-color: rgba(255, 255, 255, 0.082); position: fixed;">Loading...</h1>
<div id="frame"></div>
<video id="webcam" width="640" height="480" playsinline style="visibility: hidden; position: fixed; padding: 0px;"></video>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.4.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@0.0.1/dist/face-landmarks-detection.js"></script>
<script src="threejs/three.min.js"></script>
<script src="threejs/GLTFLoader.js"></script>
<script src="threejs/OrbitControls.js"></script>
<script>
//needed Vars
   let container, controls, camera, scene, renderer, mesh, sprite ,canvas , midx ,midy ,frameHeight ,frameWidth, loader;

// w8 for import Scripts
document.addEventListener("DOMContentLoaded", function(event) {
  init();

});


function init() {

        frameHeight = window.innerHeight - 20;
        frameWidth = window.innerWidth - 20;
        //Build empties
        container = document.getElementById( 'frame' ); 
        document.body.appendChild( container );
        camera = new THREE.PerspectiveCamera( 50, frameWidth / frameHeight, 1, 100 ); //Build always camera
        camera.position.set( 0, 0, 5 ); 
        scene = new THREE.Scene();
        scene.add(camera);//Add camera to Scene
        scene.background = new THREE.Color( 0x1200E7 );// Background of the Scene


        //Mouse input for Orbital Control
        renderer = new THREE.WebGLRenderer( { antialias: true } );
        renderer.outputEncoding = THREE.sRGBEncoding;
        renderer.setPixelRatio( window.devicePixelRatio );
        renderer.setSize( frameWidth, frameHeight );
        renderer.toneMapping = THREE.ACESFilmicToneMapping;
        renderer.outputEncoding = THREE.sRGBEncoding;
        container.appendChild( renderer.domElement );
        controls = new THREE.OrbitControls( camera, renderer.domElement );
        controls.target.set( 0, 0, 0 );// Tageting Point in scene
        controls.update();// Needed for Movement
        controls.minDistance = 2.5;
        window.addEventListener( 'resize', onWindowResize, false );


        loader = new THREE.GLTFLoader();
        checkFormat(frameWidth,frameHeight);
    }

    //Positionate Cam to Facemid Position
    function camPos(midx,midy){
        midx = (midx - 320)*-0.02;//Offset (TF coordinate - camresolution/2)Invert result (rest depends on Model) 
        midy = (midy - 240)*-0.02;//Offset (TF coordinate - camresolution/2)Invert result (rest depends on Model) 
        camera.position.set(midx,midy,5);// New camPos coordinates from Facetracking mid
        controls.update();
    }

    function checkFormat(width,height){
        if(width < 1500){
            loader.load( 'codhoch.gltf', function ( gltf ) {
            gltf.scene.scale.set(1,1,1); // Scale
            scene = new THREE.Scene();
            scene.add( gltf.scene );// Import whole scene from Blender
		    gltf.animations; // Array<THREE.AnimationClip>
		    gltf.scene; // THREE.Group
		    gltf.scenes; // Array<THREE.Group>
		    gltf.cameras; // Array<THREE.Camera>
		    gltf.asset; // Object
            animate();
        } );
        }
        else{
        
        loader.load( 'codquer.gltf', function ( gltf ) {
            gltf.scene.scale.set(1,1,1); // Scale
            scene = new THREE.Scene();
            scene.add( gltf.scene );// Import whole scene from Blender
		    gltf.animations; // Array<THREE.AnimationClip>
		    gltf.scene; // THREE.Group
		    gltf.scenes; // Array<THREE.Group>
		    gltf.cameras; // Array<THREE.Camera>
		    gltf.asset; // Object
            animate();
        } );
        }
    }
    //Orbitcontrol update window Resolution
    function onWindowResize() {
        checkFormat((window.innerWidth - 20), (window.innerHeight - 20));
        camera.aspect = (window.innerWidth - 20) / (window.innerHeight - 20);
        camera.updateProjectionMatrix();
        renderer.setSize((window.innerWidth - 20), (window.innerHeight - 20));
    }

    //Animate the Cam movement
    function animate() {
        requestAnimationFrame( animate );
        renderer.render( scene, camera);
    }

    //Loading Text for Tensorflow model
    function setText( text ) {
        document.getElementById( "status" ).innerText = text;
    }

    //Face Circle
    function drawLine( ctx, x1, y1) {
        ctx.beginPath();
        ctx.arc(x1, y1, 5, 0, 2 * Math.PI, false);
        ctx.fillStyle = 'green';
        ctx.fill();
        ctx.stroke();
        }

    //Face Mask
    function drawTriangle( ctx, x1, y1, x2, y2, x3, y3 ) {
        ctx.beginPath();
        ctx.moveTo( x1, y1 );
        ctx.lineTo( x2, y2 );
        ctx.lineTo( x3, y3 );
        ctx.lineTo( x1, y1 );
        ctx.stroke();
        }



        let output = null;
        let model = null;

        //setup Webcam Facetracking
        async function setupWebcam() {
            return new Promise( ( resolve, reject ) => {
                const webcamElement = document.getElementById( "webcam" );
                const navigatorAny = navigator;
                navigator.getUserMedia = navigator.getUserMedia ||
                navigatorAny.webkitGetUserMedia || navigatorAny.mozGetUserMedia ||
                navigatorAny.msGetUserMedia;
                if( navigator.getUserMedia ) {
                    navigator.getUserMedia( { video: true },
                        stream => {
                            webcamElement.srcObject = stream;
                            webcamElement.addEventListener( "loadeddata", resolve, false );
                        },
                    error => reject());
                }
                else {
                    reject();
                }
            });
        }

        //Track faceposition
        async function trackFace() {
            const video = document.getElementById( "webcam" );
            const faces = await model.estimateFaces( {
                input: video,
                returnTensors: false,
                flipHorizontal: false,
            });
            output.drawImage(
                video,
                0, 0, video.width, video.height,
                0, 0, video.width, video.height
            );

            faces.forEach( face => {
                

                // Draw the bounding box
                const x1 = face.boundingBox.topLeft[ 0 ];
                const y1 = face.boundingBox.topLeft[ 1 ];
                const x2 = face.boundingBox.bottomRight[ 0 ];
                const y2 = face.boundingBox.bottomRight[ 1 ];
                midx = ((x2 - x1)/2)+x1;
                midy = ((y2 - y1)/2)+y1;
                camPos(midx,midy);
                drawLine( output, midx, midy);
                
                // Draw the face mesh
//                const keypoints = face.scaledMesh;
//                for( let i = 0; i < FaceTriangles.length / 3; i++ ) {
//                    let pointA = keypoints[ FaceTriangles[ i * 3 ] ];
//                    let pointB = keypoints[ FaceTriangles[ i * 3 + 1 ] ];
//                    let pointC = keypoints[ FaceTriangles[ i * 3 + 2 ] ];
//                    drawTriangle( output, pointA[ 0 ], pointA[ 1 ], pointB[ 0 ], pointB[ 1 ], pointC[ 0 ], pointC[ 1 ] );
//                }
            });

            requestAnimationFrame( trackFace );
        }

        (async () => {
            await setupWebcam();
            const video = document.getElementById( "webcam" );
            video.play();
            let videoWidth = video.videoWidth;
            let videoHeight = video.videoHeight;
            video.width = videoWidth;
            video.height = videoHeight;

            let canvas = document.getElementById( "output" );
            canvas.width = video.width;
            canvas.height = video.height;

            output = canvas.getContext( "2d" );
            output.translate( canvas.width, 0 );
            output.scale( -1, 1 ); // Mirror cam
            output.fillStyle = "#fdffb6";
            output.strokeStyle = "#fdffb6";
            output.lineWidth = 2;

            // Load Face Landmarks Detection
            model = await faceLandmarksDetection.load(
                faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
            );

            setText( "Loaded!" );

            trackFace();
        })();
</script>
</body>